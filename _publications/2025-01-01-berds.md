---
title: "Open-World Evaluation for Retrieving Diverse Perspectives"
collection: publications
category: conferences
permalink: /publication/2025-01-01-berds
excerpt: 'We study retrieving a set of documents that covers various perspectives on a complex and contentious question (e.g., will ChatGPT do more harm than good?). We curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS), where each example consists of a question and diverse perspectives associated with the question, sourced from survey questions and debate websites. On this data, retrievers paired with a corpus are evaluated to surface a document set that contains diverse perspectives. Our framing diverges from most retrieval tasks in that document relevancy cannot be decided by simple string matches to references. Instead, we build a language model-based automatic evaluator that decides whether each retrieved document contains a perspective. This allows us to evaluate the performance of three different types of corpus (Wikipedia, web snapshot, and corpus constructed on the fly with retrieved pages from the search engine) paired with retrievers. Retrieving diverse documents remains challenging, with the outputs from existing retrievers covering all perspectives on only 33.74% of the examples. We further study the impact of query expansion and diversity-focused reranking approaches and analyze retriever sycophancy. Together, we lay the foundation for future studies in retrieval diversity handling complex queries.'
date: 2025-01-01
venue: '2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics'
venue_short: 'NAACL'
paperurl: 'https://aclanthology.org/2025.naacl-long.431/'
websiteurl: 'https://timchen0618.github.io/berds/'
citation: 'Hung-Ting Chen, Eunsol Choi. (2025). &quot;Open-World Evaluation for Retrieving Diverse Perspectives.&quot; <i>2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics</i>.'
---

This paper introduces a new evaluation framework for retrieving diverse perspectives in open-world question answering settings. We present BERDS (Benchmarking Evaluation for Retrieving Diverse Sources), which addresses the challenge of evaluating retrieval systems when the space of valid perspectives is not pre-defined.

[Website](https://timchen0618.github.io/berds/) | [Paper](https://aclanthology.org/2025.naacl-long.431/) 